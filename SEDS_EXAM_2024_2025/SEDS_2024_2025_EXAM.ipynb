{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG src=\"figures/logo-esi-sba.png\" WIDTH=300 height=\"100\" ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Software Engineering for Data Science \n",
    "## Final Exam: Fall 2024-Winter 2025\n",
    "## Exam Duration: 02 Hours\n",
    "\n",
    "*By Dr. Belkacem KHALDI (b.khaldi@esi-sba.dz)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Exam Instruction:\n",
    "\n",
    "During this exam, you are permitted to refer only to your lecture notes and lab materials. Additionally, you are authorized to access internet to only download required packages if any. Any violation of this policy will be considered a violation of academic integrity. Violations may result in immediate sanctions, including a grade of zero for the specific exercise or challenge in question. It is imperative to adhere to the specified guidelines to maintain a fair and equitable assessment environment. Your cooperation is appreciated, and any concerns or questions should be directed to the exam proctor or instructor. Thank you for your understanding and compliance.\n",
    "\n",
    "## Note: Continued Evaluation Grade:\n",
    "Half mark from this exam will be incorporated into your final Continuous Evaluation Grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Git Practice\n",
    "1. Execute the required Git commands that allows you to reach the goal (01) illustrated in the following figure:\n",
    "\n",
    "![Git Challenge 1](figures/Git_goal1.png)\n",
    "\n",
    "2. Now make the appropriet git commandes  that allows you reach  goal (02) from goal (01) shown in the following Figure. \n",
    "\n",
    "![Git Challenge 2](figures/Git_goal2.png)\n",
    "\n",
    "3. Then make the appropriet git commandes  that allows you reach  goal (03) from goal (02) shown in the following Figure. \n",
    "\n",
    "![Git Challenge 2](figures/Git_goal3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Solution here. All your git commands have to be illustrated in a Markdown cell type.\n",
    "### Include the final figure of your git log commande in the resulting Markdown cell type as image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge 2: Data Wrangling, Scraping, and Cleaning for eBooks.com\n",
    "\n",
    "\n",
    "### Objective:\n",
    "You have just taken a role as a Data Scientist at eBooks.com publishing company. Your task is to gather and clean data related to specific books from various sources. This includes scraping data for computer science-related books, querying a specific database, as well as reading CSV and JSON files. Subsequently, you will perform necessary data cleaning, data analysis, and data visualization to ensure the quality and usability of the data.\n",
    "\n",
    "\n",
    "#### Key Points to Note:\n",
    "- **Comprehensive Data Integration:** \n",
    "   You should integrate data from a CSV file, a JSON file, a SQLite database, and web scraping into one unified DataFrame.\n",
    "\n",
    "- **Data Cleaning:**\n",
    "    Emphasis should be placed on cleaning the data to ensure consistency, accuracy, and usability.\n",
    "\n",
    "- **Handling Failures:** \n",
    "   You are encouraged to perform data integration to the best of your ability, even if you fails with some of the following requirements. The final cleaned CSV file should include data from the successfully ingested sources, while clearly documenting any issues faced with the other sources in the notebook. You should save the final successfull cleaned CSV file as `\\resources\\MyeBooksComCleaned.csv`, which will be used later for `Challenge 03: Developing a Streamlit Web Application for eBooks.com`.\n",
    "   \n",
    "\n",
    "<figure>\n",
    "  <IMG src=\"figures/sqllite_db_diagram.png\"  ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "### Requirements:\n",
    "#### 1. Ingesting data from csv and json files:\n",
    "   - **CSV Files**:\n",
    "      - Read and parse the data from the CSV file (`\\resources\\ebook_data_visualization.csv`) provided by the company into a dataframe called `df_csv`  .\n",
    "      - Ensure to handle inconsistent data formats and Standardize the data formats to ensure consistency.\n",
    "   - **json Files**:\n",
    "      - Read and parse the data from a JSON file (`\\resources\\ebooks_data_json.json`) provided by the company into a dataframe called `df_json` .\n",
    "      - Ensure to handle inconsistent data formats and Standardize the data formats to ensure consistency.\n",
    "\n",
    "#### 2. Ingesting data from a SQLite Database :\n",
    "The company has provided you with its SQLite database located at `\\resources\\ebook_data_warehousing_database.db`, which contains a subset of its data. Refer to the provided database diagram for details about the database tables and their relationships.\n",
    "   - Craft an appropriate SQL query to retrieve data from the relevant tables in the provided database. \n",
    "   - Ingest the resulting data into a dataframe called `df_db`, ensuring the dataframe structure and columns are prepared for later combination with data from other company sources (CSV, JSON, web scraping).\n",
    "   - Ensure to handle inconsistent data formats and Standardize the data formats to ensure consistency.\n",
    "\n",
    "\n",
    "#### 3. Ingesting data from Web Scraping:\n",
    "<figure>\n",
    "  <IMG src=\"figures/SCRAPPING.png\"  ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "   - Use Python along with a web scraping library such as `BeautifulSoup` to scrape book details from the following (03) three local stored webpages:\n",
    "      - `\\resources\\ebook_web_pages\\Buy_Computers_Artificial_Intelligence_eBooksOnline_Page1.html`\n",
    "      - `\\resources\\ebook_web_pages\\Buy_Computers_Artificial_Intelligence_eBooksOnline_Page2.html`\n",
    "      - `\\resources\\ebook_web_pages\\Buy_Computers_Artificial_Intelligence_eBooksOnline_Page3.html`\n",
    "   -  Hints for reading local files:\n",
    "      ``` python\n",
    "         import os\n",
    "         # Create the full file path by joining the base path and the file name where:\n",
    "         # base_path is the Base directory path where the files are located\n",
    "         # and file_name is the File name to be read\n",
    "         file_path = os.path.join(<base_path>, <file_name>)\n",
    "\n",
    "         # Open the file in read mode with UTF-8 encoding\n",
    "         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Read the entire content of the file\n",
    "            content = file.read()\n",
    "      ```\n",
    "   - Scrape the details illustrated in the figure for each book into a dataframe called `df_wscrap` with the following columns names: `Title`,`Author`,`Price`,`Publishing Date`, `Book Type`, `Publisher`, and `Description`.\n",
    "      - Note that the content of the `Book Type` is a fixed value that is `Artificial Intelligence` for all scrapped items.\n",
    "      - Ensure to handle inconsistent data formats and Standardize the data formats to ensure consistency.\n",
    "      -  You may consider using the Browser Dev. Tools for further assistance and html componenets inspections.\n",
    "\n",
    "\n",
    "#### 2. Data Integration, Processing, Cleaning:\n",
    "1. **Data Validation,  Cleaning, and Integration**:\n",
    "   - Handle missing data in the dataframes.\n",
    "   - Remove any duplicate entries.\n",
    "   - Standardize columns names and the Publishing Date format to `%Y-%m`, `%m-%Y`, or `%B %Y` where:\n",
    "      - `%Y`: Year with century as a decimal number (e.g., 2025).\n",
    "      - `%m`: Month as a zero-padded decimal number (e.g., 01 for January, 12 for December).\n",
    "      - `%B`: Full month name (e.g., January, February).\n",
    "      - Examples:\n",
    "         - `%Y-%m`: This format represents a date string where the year comes first, followed by the month (e.g., 2025-01).\n",
    "         - `%m-%Y`: This format represents a date string where the month comes first, followed by the year (e.g., 01-2025).\n",
    "         - `%B %Y`: This format represents a date string where the full month name comes first, followed by the year (e.g., January 2025).\n",
    "   - Convert prices to a single currency (e.g., `US$`) using a specified exchange rate: `1 US$=0.96 Euros`.\n",
    "   - Handle any possible outliers values in the prices.\n",
    "   - Combine all dataframes into one single cleaned dataframe and save it into csv file called `\\resources\\MyeBooksComCleaned.csv`.\n",
    "\n",
    "\n",
    "#### 3. Data Analysis and Visualization:\n",
    "1. Perform basic analysis to find and visualize:\n",
    "   - The top n=5 common Publishers by Book Type with their average book prices.\n",
    "   - The top n=5 common Authors by Book Type with their average book prices.\n",
    "      \n",
    "2. Perform basic text analysis to find and visualize:\n",
    "   - The most frequent words as a word cloud from all book descriptions.\n",
    "   - The most frequent words as a word cloud from book descriptions published by a specific publisher.\n",
    "   - The most frequent words as a word cloud from book descriptions of a specific book type.\n",
    "\n",
    "3. Study and plot the probability distribution of book prices by Book Type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Solution\n",
    "\n",
    "#### Note: Additional points will be awarded for code that is well-documented, refactored, and easy to read.\n",
    "\n",
    "Please provide step-by-step code for each part of the required challenge task..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chalenge 03: Developing a Streamlit Web Application for eBooks.com\n",
    "\n",
    "Using your cleaned CSV file `\\resources\\MyeBooksComCleaned.csv`, you are tasked with creating a Streamlit web application that includes the following functionalities:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**:\n",
    "   - Perform basic EDA on the dataset to understand its structure and key features.\n",
    "\n",
    "2. **Basic Analysis and Visualization**:\n",
    "   - Identify and visualize the top 5 common publishers by book type along with their average book prices.\n",
    "   - Identify and visualize the top 5 common authors by book type along with their average book prices.\n",
    "\n",
    "3. **Text Analysis and Visualization**:\n",
    "   - Generate and visualize a word cloud of the most frequent words from all book descriptions.\n",
    "   - Generate and visualize a word cloud of the most frequent words from book descriptions published by a specific publisher.\n",
    "   - Generate and visualize a word cloud of the most frequent words from book descriptions of a specific book type.\n",
    "\n",
    "4. **Probability Distribution Analysis**:\n",
    "   - Analyze and plot the probability distribution of book prices by book type.\n",
    "\n",
    "5. **Additional Functionalities**:\n",
    "   - Allow users to upload their own CSV file for analysis.\n",
    "   - Organize the Streamlit app into separate tabs or pages for each functionality to improve user experience and navigation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Solution:\n",
    "### Include screenshots of your Streamlit Web App in Markdown cell type as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chalenge 04: Code Refactoring Techniques\n",
    "### part one:\n",
    "What refactoring techniques can be applied to the following Python code to enhance its readability? Please highlight the main issues and suggest specific refactoring methods to address them.\n",
    "1. #### Code 1:\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Calculate mean and median of a column\n",
    "    mean_value = df['column'].mean()\n",
    "    median_value = df['column'].median()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Mean: {mean_value}, Median: {median_value}\")\n",
    "    ```\n",
    "\n",
    "2. #### Code 2:\n",
    "   \n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('data.csv')\n",
    "\n",
    "    # Filter the data\n",
    "    f = data[data['column'] > 10]\n",
    "\n",
    "    # Calculate the sum\n",
    "    s = f['column'].sum()\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Sum: {s}\")\n",
    "    ```    \n",
    "\n",
    "\n",
    "### Part Two:\n",
    "You are required to create a Python package that includes the refactored code from the two above provided code snippets. Ensure that the package is well-documented, structured, and does not rely on fixed file name strings, fixed dataframe columns names, or fixed parameters. Additionally, test the installation of this package using an offline installation method\n",
    "The results of the offline installation test should be demonstrated here in a Jupyter notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks:\n",
    "\n",
    "1. Zip the entire folder that contains your jupyther notebook, and git snapshots, as well as your streamlit folder in one file named as your esi-sba  email \n",
    "2. Send the compressed file to b.khaldi@esi-sba.dz"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
